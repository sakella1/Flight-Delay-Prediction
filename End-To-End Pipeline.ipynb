{"cells":[{"cell_type":"markdown","source":["## End to End Pipeline for Machine Learning"],"metadata":{}},{"cell_type":"markdown","source":["ML pipeline created to predict flight delays 2 hours prior to departure using Gradient Boosting algorithm. The pipeline will transform the data and output a prediction. \n\nInput:\n- Airline passenger flights data path\n- Weather data path\n\nOutput:\n- Confusion matrix for test data using Gradient Boosting Algorithm"],"metadata":{}},{"cell_type":"markdown","source":["### Import packages"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as f\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, NullType, ShortType, DateType, BooleanType, BinaryType\nfrom pyspark.sql import SQLContext\nimport pyspark.ml.feature as ftr\nimport pyspark.ml as ml\nfrom pyspark.sql.window import Window\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.pipeline import PipelineModel\nfrom pyspark.ml.classification import GBTClassifier\nimport numpy as np\nimport pandas as pd\nimport sklearn.metrics as metrics\nimport matplotlib.pyplot as plt\nsqlContext = SQLContext(sc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/sql/context.py:77: DeprecationWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  DeprecationWarning)\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["### Airport Data"],"metadata":{}},{"cell_type":"code","source":["def airport_extract(airport_path):\n  \n  \"\"\"\n  Extract airport data from the specified path\n  \n  Parameters:\n  airport_path: path to the airport data file\n  \n  Returns:\n  df: airport dataframe  \n  \"\"\"\n\n  file_location = airport_path\n  file_type = \"csv\"\n\n  infer_schema = \"true\"\n  first_row_is_header = \"true\"\n  delimiter = \",\"\n  \n  df = None\n  df = spark.read.format(file_type).option(\"inferSchema\", infer_schema).option(\"header\", first_row_is_header).option(\"sep\", delimiter).load(file_location)\n  return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["##### Transforming airport Data"],"metadata":{}},{"cell_type":"markdown","source":["Filter down to US airports only"],"metadata":{}},{"cell_type":"code","source":["def airport_transform(dataframe):\n     \n  \"\"\"\n  Filters airport data down to US airport only\n  Remove duplicates (original dataset uses polygon to represent some airports; one point each airport is sufficient for the model)\n  \n  Parameters:\n  dataframe: airport data\n  \n  Returns:\n  dataframe: airport data   \n  \"\"\"\n\n  return (\n    dataframe\n    .filter(\"AIRPORT_COUNTRY_CODE_ISO = 'US'\")\n    .dropDuplicates(['AIRPORT'])\n    )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Keep airports that co-exist in the airline dataset"],"metadata":{}},{"cell_type":"code","source":["def airport_selection(airport_df, airlines_df):\n  \n  \"\"\"\n  Filters airport data down to airport that coexists in airlines dataset\n    \n  Parameters:\n  airport_df: airport data\n  airlines_df: airlines data\n  \n  Returns:\n  df: airport data   \n  \"\"\"\n\n  airlines_df.createOrReplaceTempView('airlines')\n  airport_df.createOrReplaceTempView('airport')\n\n  df = None\n  df = spark.sql(\n    \"\"\"\n    SELECT * \n    FROM\n      airport \n    WHERE \n      AIRPORT IN (\n        SELECT DISTINCT ORIGIN FROM airlines\n        UNION\n        SELECT DISTINCT DEST FROM airlines\n        )\n    \"\"\"\n  )\n  \n  return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["##### Transforming Airline Data"],"metadata":{}},{"cell_type":"code","source":["def airlines_transform(dataframe):\n  \n  \"\"\"\n  Airlines transformation function\n    - Drop cancelled and diverted flights\n    - cast FL_DATE to date type\n    - cast OP_CARRIER_FL_NUM to string type\n    - create new column DEP_TIME_HOUR from DEP_TIME_BLK\n    - create new column ARR_TIME_HOUR from ARR_TIME_BLK\n    - extract scheduled departure hour CRS_DEP_TIME_HOUR from CRS_DEP_TIME\n    - extract scheduled arrival hour CRS_ARR_TIME_HOUR from CRS_ARR_TIME\n    - cast DISTANCE_GROUP to string type\n    - Create new column by concatenating OP_CARRIER and OP_CARRIER_FL_NUMBER\n    - cast DEP_DEL15 to string type\n    - creates a new timestamp column and a two hour prior timestamp column\n    - creates a new prior flight delay column\n    \n  Parameters:\n  dataframe: airlines data\n  \n  Returns:\n  dataframe: airlines data   \n  \"\"\"\n  \n  # Selected Columns\n  selected_col = [\n  \"YEAR\",\n  \"QUARTER\",\n  \"MONTH\",\n  \"DAY_OF_MONTH\",\n  \"DAY_OF_WEEK\",\n  \"FL_DATE\",\n  \"OP_UNIQUE_CARRIER\",\n  \"TAIL_NUM\",\n  \"OP_CARRIER_FL_NUM\",\n  \"ORIGIN\",\n  \"ORIGIN_CITY_NAME\",\n  \"ORIGIN_STATE_ABR\",\n  \"DEST\",\n  \"DEST_CITY_NAME\",\n  \"DEST_STATE_ABR\",\n  \"CRS_DEP_TIME\",\n  \"CRS_DEP_TIME_HOUR\",\n  'CRS_ARR_TIME_HOUR',\n  \"DEP_TIME_HOUR\",\n  \"DEP_DELAY_NEW\",\n  \"DISTANCE\",\n  \"DISTANCE_GROUP\",\n  \"DEP_DEL15\",\n  \"ORIGIN_AIRPORT_ID\",\n  \"DEST_AIRPORT_ID\",\n  \"CRS_DEP_TIMESTAMP\",\n  \"CRS_ARR_TIMESTAMP\",\n  \"PR_ARR_DEL15\"]\n  \n  # Creating a window partition to extract prior arrival delay for each flight\n  windowSpec = Window.partitionBy(\"TAIL_NUM\").orderBy(\"CRS_DEP_TIMESTAMP\")\n  \n  # Returning the transformed dataset\n  return (\n    dataframe\n    .filter(\"CANCELLED != 1 AND DIVERTED != 1\")\n    .withColumn(\"FL_DATE\", f.col(\"FL_DATE\").cast(\"date\"))\n    .withColumn(\"OP_CARRIER_FL_NUM\", f.col(\"OP_CARRIER_FL_NUM\").cast(\"string\"))\n    .withColumn(\"DEP_TIME_HOUR\", dataframe.DEP_TIME_BLK.substr(1, 2).cast(\"int\"))\n    .withColumn(\"ARR_TIME_HOUR\", dataframe.ARR_TIME_BLK.substr(1, 2).cast(\"int\"))\n    .withColumn(\"CRS_DEP_TIME_HOUR\", f.round((f.col(\"CRS_DEP_TIME\")/100)).cast(\"int\"))\n    .withColumn(\"CRS_ARR_TIME_HOUR\", f.round((f.col(\"CRS_ARR_TIME\")/100)).cast(\"int\"))\n    .withColumn(\"DISTANCE_GROUP\", f.col(\"DISTANCE_GROUP\").cast(\"string\"))\n    .withColumn(\"OP_CARRIER_FL_NUM\", f.concat(f.col(\"OP_CARRIER\"),f.lit(\"_\"),f.col(\"OP_CARRIER_FL_NUM\")))\n    .withColumn(\"DEP_DEL15\", f.col(\"DEP_DEL15\").cast(\"string\"))\n    .withColumn(\"ARR_DEL15\", f.col(\"ARR_DEL15\").cast(\"string\"))\n    .withColumn(\"FL_DATE_string\", f.col(\"FL_DATE\").cast(\"string\"))\n    .withColumn(\"YEAR\", f.col(\"YEAR\").cast(\"string\"))\n    .withColumn(\"QUARTER\", f.col(\"QUARTER\").cast(\"string\"))\n    .withColumn(\"MONTH\", f.col(\"MONTH\").cast(\"string\"))\n    .withColumn(\"DAY_OF_MONTH\", f.col(\"DAY_OF_MONTH\").cast(\"string\"))\n    .withColumn(\"DAY_OF_WEEK\", f.col(\"DAY_OF_WEEK\").cast(\"string\"))\n    .withColumn(\"CRS_DEP_TIME_string\", f.col(\"CRS_DEP_TIME\").cast(\"string\"))\n    .withColumn(\"CRS_ARR_TIME_string\", f.col(\"CRS_ARR_TIME\").cast(\"string\"))\n    .withColumn(\"CRS_DEP_TIME_HOUR_string\", f.col(\"CRS_DEP_TIME_HOUR\").cast(\"string\"))\n    .withColumn(\"CRS_ARR_TIME_HOUR_string\", f.col(\"CRS_ARR_TIME_HOUR\").cast(\"string\"))\n    .withColumn(\"CRS_DEP_TIME_HH\", f.lpad(\"CRS_DEP_TIME_string\", 4, '0').substr(1,2))\n    .withColumn(\"CRS_DEP_TIME_MM\", f.lpad(\"CRS_DEP_TIME_string\", 4, '0').substr(3,2))\n    .withColumn(\"CRS_ARR_TIME_HH\", f.lpad(\"CRS_ARR_TIME_string\", 4, '0').substr(1,2))\n    .withColumn(\"CRS_ARR_TIME_MM\", f.lpad(\"CRS_ARR_TIME_string\", 4, '0').substr(3,2))\n    # Timestamp created to calculate prior arrival delay of a flight and to join with weather dataset\n    .withColumn(\"CRS_DEP_TIMESTAMP\", f.concat(f.col(\"FL_DATE_string\"),f.lit(\" \"),f.col(\"CRS_DEP_TIME_HH\"), f.lit(\":\"),f.col(\"CRS_DEP_TIME_MM\")).cast(\"timestamp\"))\n    .withColumn(\"CRS_ARR_TIMESTAMP\", f.concat(f.col(\"FL_DATE_string\"),f.lit(\" \"),f.col(\"CRS_ARR_TIME_HH\"), f.lit(\":\"),f.col(\"CRS_ARR_TIME_MM\")).cast(\"timestamp\"))\n    .withColumn(\"CRS_ELAPSED_TIME\", f.round((f.col(\"CRS_ELAPSED_TIME\")/60)).cast(\"int\"))\n    # Variable created to incorportate effects of a prior arrival delay into the current flight's departure delay\n    .withColumn(\"PR_ARR_DEL15\", f.lag(f.col(\"ARR_DEL15\"), 1).over(windowSpec).cast(\"string\"))\n    .select(selected_col)\n    )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["def weather_extraction(weather_path):\n  \n  \"\"\"\n  Extract weather data from the specified path\n  \n  Parameters:\n  weather_path: path to the weather data file\n  \n  Returns:\n  df: weather dataframe\n  \"\"\"\n    \n  weather_df = spark.read.option(\"header\", \"true\").parquet(weather_path)\n  return weather_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["##### Transforming weather data - part 1\n- Filter weather dataset to US and Report Type \"FM-15\""],"metadata":{}},{"cell_type":"code","source":["def weather_transformation_reduction(dataframe, shorlisted_weather_cols):\n  \n  \"\"\"\n  Filters weather data:\n    - US geographical location only\n    - weather records for FM15 report type\n    - remove columns not shortlisted for model input\n  \n  Parameters:\n  dataframe: weather data\n  shorlisted_weather_cols: columns shortlisted for model inputs or for feature engineering\n  \n  Returns:\n  dataframe: weather data   \n  \"\"\"\n  \n  return (\n    dataframe\n      .withColumn(\"COUNTRY\", f.substring(f.col(\"NAME\"), -2, 2))\n      .filter(\"COUNTRY = 'US'\")\n      .filter(\"REPORT_TYPE LIKE '%FM-15%'\")\n      .select(shorlisted_weather_cols)\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["##### Transforming weather data - part 2\n- Keep station records that co-exist in the airport dataset"],"metadata":{}},{"cell_type":"code","source":["def weather_coordinates_extract(weather_df):\n  \n  \"\"\"\n  Filters weather data by weather stations that coexist in the airport data\n  \n  Parameters:\n  weather_df: weather data\n  \n  Returns:\n  weather_coordinates: weather station coordinates \n  \"\"\"\n  \n  weather_df.createOrReplaceTempView('weather')\n  \n  weather_coordinates = spark.sql(\n  \"\"\"\n  SELECT \n    DISTINCT STATION, CALL_SIGN, LATITUDE, LONGITUDE\n  FROM \n    weather\n  \"\"\"\n  )\n  \n  return weather_coordinates"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["def spatial_join(weather_coordinates_df, airport_df, weather_df):\n  \n  \"\"\"\n  spatial join to obtain weather station based on euclidean distance\n  \n  Parameters:\n  weather_coordinates_df: weather station location coordinates\n  airport_df: airport data\n  weather_df: weather data\n  \n  Returns:\n  airport_df: airport data\n  weather_df: weather data\n  \"\"\"\n  \n  weather_coordinates_pdf = weather_coordinates_df.toPandas()\n  airport_pdf = airport_df.toPandas()\n\n  X_coordinates = airport_pdf[['LATITUDE', 'LONGITUDE']]\n  Y_coordinates = weather_coordinates_pdf[['LATITUDE', 'LONGITUDE']]\n\n  weather_station_idx = metrics.pairwise_distances_argmin_min(X_coordinates, Y_coordinates, metric='euclidean')[0]\n\n  station_id = [weather_coordinates_pdf.iloc[i]['STATION'] for i in weather_station_idx]\n  station_id_weather_filter = spark.createDataFrame(station_id,StringType())\n  station_id_weather_filter.createOrReplaceTempView('station_id_weather_filter')\n  \n  airport_pdf['weather_station_id'] = station_id\n  airport_df = spark.createDataFrame(airport_pdf)\n\n  weather_df = weather_df.where(f.col(\"STATION\").isin(set(station_id)))\n  return (weather_df, airport_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["##### Transforming weather data - part 3\n- Weather feature extraction and tarnsformation\n\n1. First of all, by looking into each weather feature, we extract relevant features that would affect to the airline delay.\n2. In the weather dataset, there are missing values that can't count as Null. In order to handle those values for imputation later, we filled those missing values with Null.\n3. In weather data section, there are substrings delimited by \",\", so we parsed out those substrings into new columns.\n4. We assigned erroneous data which codes are 3 and 7 to \"999\" which indicates missing values.\n5. We changed all missing values with Null which will be handled with imputation later.\n6. Finally we dropped unnecessary columns for faster running."],"metadata":{}},{"cell_type":"code","source":["def weather_transformation(dataframe):\n  \n  \"\"\"\n  Weather feature extraction and transformation\n  \n  Parameters:\n  dataframe: weather data\n  \n  Returns:\n  dataframe: weather data\n  \"\"\"\n  \n  return (\n    dataframe\n      # Mandatory data section - WND - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"WND_temp\", f.substring_index(\"WND\", \",\", -2))\\\n      .withColumn(\"WND_SPEED\", f.substring_index(\"WND_temp\", \",\", 1))\\\n      .withColumn(\"WND_SPEED_QUALITY\", f.substring_index(\"WND_temp\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"WND_SPEED_QUALITY\", f.when((f.col(\"WND_SPEED_QUALITY\") == \"3\") | (f.col(\"WND_SPEED_QUALITY\") == \"7\") , \"999\").otherwise(f.col(\"WND_SPEED_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"WND_SPEED\", f.when((f.col(\"WND_SPEED\") == \"\") | (f.col(\"WND_SPEED\") == \"9999\") | (f.col(\"WND_SPEED_QUALITY\") == \"999\"), None).otherwise(f.col(\"WND_SPEED\")))\\\n      # Drop unnecessary columns\n      .drop(\"WND_temp\",\"WND\", \"WND_SPEED_QUALITY\")\\\n      # Mandatory data section - CIG - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"CIG_CAVOC\", f.substring_index(\"CIG\", \",\", -1))\\\n      # Change the missing values to Null\n      .withColumn(\"CIG_CAVOC\", f.when((f.col(\"CIG_CAVOC\") == \"\") | (f.col(\"CIG_CAVOC\") == \"9\"), None).otherwise(f.col(\"CIG_CAVOC\")))\\\n      # Drop unnecessary columns\n      .drop(\"CIG\")\\\n      # Mandatory data section - VIS - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"VIS_temp\", f.substring_index(\"VIS\", \",\", 2))\\\n      .withColumn(\"VIS_DISTANCE\", f.substring_index(\"VIS_temp\", \",\", 1))\\\n      .withColumn(\"VIS_DISTANCE_QUALITY\", f.substring_index(\"VIS_temp\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"VIS_DISTANCE_QUALITY\", f.when((f.col(\"VIS_DISTANCE_QUALITY\") == \"3\") | (f.col(\"VIS_DISTANCE_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"VIS_DISTANCE_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"VIS_DISTANCE\", f.when((f.col(\"VIS_DISTANCE\") == \"\") | (f.col(\"VIS_DISTANCE\") == \"999999\") | (f.col(\"VIS_DISTANCE_QUALITY\") == \"999\"), None).otherwise(f.col(\"VIS_DISTANCE\")))\\\n      # Drop unnecessary columns\n      .drop(\"VIS_temp\", \"VIS_DISTANCE_QUALITY\", \"VIS\")\\\n      # Mandatory data section - TMP - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"TMP_TEMP\", f.substring_index(\"TMP\", \",\", 1))\\\n      .withColumn(\"TMP_TEMP_QUALITY\", f.substring_index(\"TMP\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"TMP_TEMP_QUALITY\", f.when((f.col(\"TMP_TEMP_QUALITY\") == \"3\") | (f.col(\"TMP_TEMP_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"TMP_TEMP_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"TMP_TEMP\", f.when((f.col(\"TMP_TEMP\") == \"\") | (f.col(\"TMP_TEMP\") == \"+9999\") | (f.col(\"TMP_TEMP_QUALITY\") == \"999\"), None).otherwise(f.col(\"TMP_TEMP\")))\\\n      # Drop unnecessary columns\n      .drop(\"TMP_TEMP_QUALITY\", \"TMP\")\\\n      # Mandatory data section - DEW - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"DEW_TEMP\", f.substring_index(\"DEW\", \",\", 1))\\\n      .withColumn(\"DEW_TEMP_QUALITY\", f.substring_index(\"DEW\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"DEW_TEMP_QUALITY\", f.when((f.col(\"DEW_TEMP_QUALITY\") == \"3\") | (f.col(\"DEW_TEMP_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"DEW_TEMP_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"DEW_TEMP\", f.when((f.col(\"DEW_TEMP\") == \"\") | (f.col(\"DEW_TEMP\") == \"+9999\") | (f.col(\"DEW_TEMP_QUALITY\") == \"999\"), None).otherwise(f.col(\"DEW_TEMP\")))\\\n      # Drop unnecessary columns\n      .drop(\"DEW_TEMP_QUALITY\", \"DEW\")\\\n      # Mandatory data section - SLP - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"SLP_PRESSURE\", f.substring_index(\"SLP\", \",\", 1))\\\n      .withColumn(\"SLP_PRESSURE_QUALITY\", f.substring_index(\"SLP\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"SLP_PRESSURE_QUALITY\", f.when((f.col(\"SLP_PRESSURE_QUALITY\") == \"3\") | (f.col(\"SLP_PRESSURE_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"SLP_PRESSURE_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"SLP_PRESSURE\", f.when((f.col(\"SLP_PRESSURE\") == \"\") | (f.col(\"SLP_PRESSURE\") == \"99999\") | (f.col(\"SLP_PRESSURE_QUALITY\") == \"999\"), None).otherwise(f.col(\"SLP_PRESSURE\")))\\\n      # Drop unnecessary columns\n      .drop(\"SLP_PRESSURE_QUALITY\", \"SLP\" )\\\n      # Additional data section - AA1 - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"AA1_temp\", f.substring_index(\"AA1\", \",\", -3))\\\n      .withColumn(\"PRECIPITATION\", f.substring_index(\"AA1_temp\", \",\", 1))\\\n      .withColumn(\"PRECIPITATION_QUALITY\", f.substring_index(\"AA1_temp\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"PRECIPITATION_QUALITY\", f.when((f.col(\"PRECIPITATION_QUALITY\") == \"3\") | (f.col(\"PRECIPITATION_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"PRECIPITATION_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"PRECIPITATION\", f.when((f.col(\"PRECIPITATION\") == \"\") | (f.col(\"PRECIPITATION\") == \"9999\") | (f.col(\"PRECIPITATION_QUALITY\") == \"999\"), None).otherwise(f.col(\"PRECIPITATION\")))\\\n      # Drop unnecessary columns\n      .drop(\"AA1_temp\", \"AA1\", \"PRECIPITATION_QUALITY\")\\\n      # Additional data section - AJ1 - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"AJ1_temp\", f.substring_index(\"AJ1\", \",\", 3))\\\n      .withColumn(\"SNOW\", f.substring_index(\"AJ1_temp\", \",\", 1))\\\n      .withColumn(\"SNOW_QUALITY\", f.substring_index(\"AJ1_temp\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"SNOW_QUALITY\", f.when((f.col(\"SNOW_QUALITY\") == \"3\") | (f.col(\"SNOW_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"SNOW_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"SNOW\", f.when((f.col(\"SNOW\") == \"\") | (f.col(\"SNOW\") == \"9999\") | (f.col(\"SNOW_QUALITY\") == \"999\"), None).otherwise(f.col(\"SNOW\")))\\\n      # Drop unnecessary columns\n      .drop(\"AJ1_temp\", \"AJ1\", \"SNOW_QUALITY\")\\\n      # Additional data section - AT1 - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"AT1_temp\", f.substring_index(\"AT1\", \",\", -3))\\\n      .withColumn(\"WEATHER_OBSERVATION\", f.substring_index(\"AT1_temp\", \",\", 1))\\\n      .withColumn(\"WEATHER_OBSERVATION_QUALITY\", f.substring_index(\"AT1_temp\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"WEATHER_OBSERVATION_QUALITY\", f.when((f.col(\"WEATHER_OBSERVATION_QUALITY\") == \"3\") | (f.col(\"WEATHER_OBSERVATION_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"WEATHER_OBSERVATION_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"WEATHER_OBSERVATION\", f.when((f.col(\"WEATHER_OBSERVATION\") == \"\") | (f.col(\"WEATHER_OBSERVATION_QUALITY\") == \"999\"), None).otherwise(f.col(\"WEATHER_OBSERVATION\")))\\\n      # Drop unnecessary columns\n      .drop(\"AT1\", \"AT1_temp\", \"WEATHER_OBSERVATION_QUALITY\")\\\n      # Additional data section - GA1 - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"GA1_temp\", f.substring_index(\"GA1\", \",\", 4))\\\n      .withColumn(\"GA1_temp2\", f.substring_index(\"GA1_temp\", \",\", 2))\\\n      .withColumn(\"GA1_temp3\", f.substring_index(\"GA1_temp\", \",\", -2))\\\n      .withColumn(\"CLOUD_COVERAGE\", f.substring_index(\"GA1_temp2\", \",\", 1))\\\n      .withColumn(\"CLOUD_COVERAGE_QUALITY\", f.substring_index(\"GA1_temp2\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"CLOUD_COVERAGE_QUALITY\", f.when((f.col(\"CLOUD_COVERAGE_QUALITY\") == \"3\") | (f.col(\"CLOUD_COVERAGE_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"CLOUD_COVERAGE_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"CLOUD_COVERAGE\", f.when((f.col(\"CLOUD_COVERAGE\") == \"\") | (f.col(\"CLOUD_COVERAGE\") == \"99\") | (f.col(\"CLOUD_COVERAGE\") == \"9\") | (f.col(\"CLOUD_COVERAGE\") == \"10\") | (f.col(\"CLOUD_COVERAGE_QUALITY\") == \"999\"), None).otherwise(f.col(\"CLOUD_COVERAGE\")))\\\n      # Drop unnecessary columns\n      .drop(\"GA1\", \"GA1_temp\", \"GA1_temp2\", \"CLOUD_COVERAGE_QUALITY\")\\\n      # Additional data section - GA1 - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"CLOUD_BASE_HEIGHT\", f.substring_index(\"GA1_temp3\", \",\", 1))\\\n      .withColumn(\"CLOUD_BASE_HEIGHT_QUALITY\", f.substring_index(\"GA1_temp3\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"CLOUD_BASE_HEIGHT_QUALITY\", f.when((f.col(\"CLOUD_BASE_HEIGHT_QUALITY\") == \"3\") | (f.col(\"CLOUD_BASE_HEIGHT_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"CLOUD_BASE_HEIGHT_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"CLOUD_BASE_HEIGHT\", f.when((f.col(\"CLOUD_BASE_HEIGHT\") == \"\") | (f.col(\"CLOUD_BASE_HEIGHT\") == \"+99999\") | (f.col(\"CLOUD_BASE_HEIGHT_QUALITY\") == \"999\"), None).otherwise(f.col(\"CLOUD_BASE_HEIGHT\")))\\\n      # Drop unnecessary columns\n      .drop(\"GA1_temp3\", \"CLOUD_BASE_HEIGHT_QUALITY\")\\\n      # Additional data section - IA1 - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"GROUND_SURFACE\", f.substring_index(\"IA1\", \",\", 1))\\\n      .withColumn(\"GROUND_SURFACE_QUALITY\", f.substring_index(\"IA1\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"GROUND_SURFACE_QUALITY\", f.when(f.col(\"GROUND_SURFACE_QUALITY\") == \"3\", \"999\").otherwise(f.col(\"GROUND_SURFACE_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"GROUND_SURFACE\", f.when((f.col(\"GROUND_SURFACE\") == \"\") | (f.col(\"GROUND_SURFACE\") == \"99\") | (f.col(\"GROUND_SURFACE_QUALITY\") == \"999\"), None).otherwise(f.col(\"GROUND_SURFACE\")))\\\n      # Drop unnecessary columns\n      .drop(\"IA1\", \"GROUND_SURFACE_QUALITY\" )\\\n      # Additional data section - MA1 - Create another substring column from the column that has multiple values delimited by \",\"\n      .withColumn(\"MA1_temp\", f.substring_index(\"MA1\", \",\", 2))\\\n      .withColumn(\"ALTIMETER_SET\", f.substring_index(\"MA1_temp\", \",\", 1))\\\n      .withColumn(\"ALTIMETER_SET_QUALITY\", f.substring_index(\"MA1_temp\", \",\", -1))\\\n      # Filter out erroneous data\n      .withColumn(\"ALTIMETER_SET_QUALITY\", f.when((f.col(\"ALTIMETER_SET_QUALITY\") == \"3\") | (f.col(\"ALTIMETER_SET_QUALITY\") == \"7\"), \"999\").otherwise(f.col(\"ALTIMETER_SET_QUALITY\")))\\\n      # Fill/Change the missing values to Null\n      .withColumn(\"ALTIMETER_SET\", f.when((f.col(\"ALTIMETER_SET\") == \"\") | (f.col(\"ALTIMETER_SET\") == \"99999\") | (f.col(\"ALTIMETER_SET_QUALITY\") == \"999\"), None).otherwise(f.col(\"ALTIMETER_SET\")))\\\n      # Drop unnecessary columns\n      .drop(\"MA1\", \"MA1_temp\", \"ALTIMETER_SET_QUALITY\")\n  )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["##### Transforming weather data - part 4\n- Weather data UTC time to local time conversion to match with airline dataset which uses local time zone"],"metadata":{}},{"cell_type":"code","source":["# Getting time zone for each weather station\ndef get_timezone(longitude, latitude):\n    from timezonefinder import TimezoneFinder\n    tzf = TimezoneFinder()\n    return tzf.timezone_at(lng=longitude, lat=latitude)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["##### Create timestamp"],"metadata":{}},{"cell_type":"code","source":["def create_timestamp(airlines_df, weather_df):\n  \n  \"\"\"\n  Create weather timestamp and two-hour prior timestamps for airlines based on scheduled departure times for joining the two datasets\n  \n  Parameters:\n  airlines_df: airlines data\n  weather_df: weather data\n  \n  Returns:\n  airlines_df: airlines data\n  weather_df: weather data\n  \"\"\"\n\n  airlines_df = airlines_df.withColumn('TWO_HOUR', airlines_df.CRS_DEP_TIMESTAMP + f.expr('INTERVAL -2 HOURS'))\n  \n  # Weather - Create timestamp using date/hour/minute part\n  udf_timezone = f.udf(get_timezone, StringType()) \n  weather_df = weather_df.withColumn(\"LOCAL\", udf_timezone(weather_df.LONGITUDE, weather_df.LATITUDE))\n  weather_df = weather_df.withColumn(\"LOCAL_DATE\", f.from_utc_timestamp(f.col(\"DATE\"), weather_df.LOCAL))\n  \n  weather_df = weather_df.withColumn(\"DATE_PART\", f.to_date(f.col(\"LOCAL_DATE\")))\\\n         .withColumn(\"HOUR_PART\", f.hour(f.col(\"LOCAL_DATE\")).cast(\"int\"))\\\n         .withColumn(\"MINUTE_PART\", f.minute(f.col(\"LOCAL_DATE\")).cast(\"int\"))\\\n         .withColumn(\"CALL_SIGN\", f.trim(f.col(\"CALL_SIGN\")))\\\n         .drop(\"LOCAL\", \"LOCAL_DATE\")\n  \n  weather_df = weather_df.withColumn(\"AL_JOIN_DATE\", f.col(\"DATE_PART\").cast(\"string\"))\\\n                         .withColumn(\"AL_JOIN_HOUR\", f.col(\"HOUR_PART\").cast(\"string\"))\\\n                         .withColumn(\"AL_JOIN_MINUTE\", f.col(\"MINUTE_PART\").cast(\"string\"))\\\n                         .withColumn(\"AL_JOIN_TIMESTAMP\", f.concat(f.col(\"AL_JOIN_DATE\"),f.lit(\" \"),f.col(\"AL_JOIN_HOUR\"), f.lit(\":\"),f.col(\"AL_JOIN_MINUTE\")).cast(\"timestamp\"))\n  \n  # Creating a helper timestamp column for the airlines to weather join\n  # Converting timestamp to unix\n  # Reducing ThisTimeStamp be a second to avoid overlapping ranges during join\n  windowSpecJoin = Window.partitionBy(\"STATION\").orderBy(\"AL_JOIN_TIMESTAMP\")\n  weather_df = weather_df.withColumn(\"ThisTimeStamp\", f.unix_timestamp(f.col(\"AL_JOIN_TIMESTAMP\"))).withColumn(\"NextTimeStamp\", f.lead(f.col(\"ThisTimeStamp\"), 1).over(windowSpecJoin) -1)\n  \n  # Converting airlines timestamp to unix for airlines to weather join.\n  airlines_df = airlines_df.withColumn(\"AL_ThisTimeStamp\", f.unix_timestamp(f.col(\"TWO_HOUR\")))\n  return (airlines_df, weather_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["##### Airport to airline join"],"metadata":{}},{"cell_type":"code","source":["def airport_airline_join(airlines_df, airport_df):\n  \n  \"\"\"\n  Left join of airport data with airline dataset\n  \n  Parameters:\n  airlines_df: airlines data\n  airport_df: airport data\n  \n  Returns:\n  airlines_df: airlines data\n  \"\"\"\n  \n  airlines_df.createOrReplaceTempView('airlines')\n  airport_df.createOrReplaceTempView('airport')\n\n  airlines_df = spark.sql(\n  \"\"\"\n  SELECT \n    airline.*,\n    airport_origin.LATITUDE AS ORIGIN_LATITUDE,\n    airport_origin.LONGITUDE AS ORIGIN_LONGITUDE,\n    airport_origin.weather_station_id AS weather_station,\n    airport_destination.LATITUDE AS DESTINATION_LATITUDE,\n    airport_destination.LONGITUDE AS DESTINATION_LONGITUDE\n  FROM\n    airlines airline\n    LEFT JOIN airport airport_origin\n      ON airline.ORIGIN = airport_origin.AIRPORT\n    LEFT JOIN airport airport_destination\n      ON airline.DEST = airport_destination.AIRPORT\n  \"\"\"\n  )\n  return airlines_df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["def airlines_weather_leftjoin(airlines_df, weather_df):\n  \n  \"\"\"\n  Left join of weather data with airline dataset\n  \n  Parameters:\n  airlines_df: airlines data\n  weather_df: weather data\n  \n  Returns:\n  airlines_df: airlines and weather joined data\n  \"\"\"\n\n  airlines_df.repartition(363, \"weather_station\").createOrReplaceTempView('airlines')\n  weather_df.repartition(363, \"STATION\").createOrReplaceTempView('weather')\n\n\n  airlines_weather = spark.sql(\"\"\"\n  SELECT a.*, w.*\n  FROM\n    airlines a\n  LEFT JOIN weather w\n  ON\n    a.weather_station = w.STATION\n    AND a.AL_ThisTimeStamp BETWEEN w.ThisTimeStamp AND w.NextTimeStamp\n    \"\"\"\n  )\n  \n  return airlines_weather"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["##### Create data model"],"metadata":{}},{"cell_type":"code","source":["def create_data_model(dataframe):\n  \n  \"\"\"\n  Preparing the data for inout into the model for training and testing\n    - Keeping only the required features\n    - Ensuring the dataType is accurate\n    - imputing values based on the feature distribution\n  \n  Parameters:\n  dataframe: airlines and weather joined data\n  \n  Returns:\n  dataframe: dataset for model training and testing\n  \"\"\"\n  \n  selected_columns = ['YEAR',\n   'DAY_OF_WEEK',\n   'ORIGIN',\n   'DEST',\n   'CRS_ARR_TIME_HOUR',\n   'CRS_DEP_TIME_HOUR',\n   'OP_UNIQUE_CARRIER',\n   'MONTH',\n   'DAY_OF_MONTH',\n   'QUARTER',\n   'PR_ARR_DEL15',\n   'CLOUD_BASE_HEIGHT',\n   'ALTIMETER_SET',\n   'PRECIPITATION',\n   'SNOW',\n   'SLP_PRESSURE',\n   'TMP_TEMP',\n   'DEW_TEMP',\n   'DISTANCE',\n   'CLOUD_COVERAGE',\n   'VIS_DISTANCE',\n   'WND_SPEED',\n   'DEP_DEL15']\n  \n  selected_columns = dataframe.select(selected_columns)\n  \n  categoricals = [\n   'YEAR',\n   'DAY_OF_WEEK',\n   'ORIGIN',\n   'DEST',\n   'CRS_ARR_TIME_HOUR',\n   'CRS_DEP_TIME_HOUR',\n   'OP_UNIQUE_CARRIER',\n   'MONTH',\n   'DAY_OF_MONTH',\n   'QUARTER',\n   'PR_ARR_DEL15',\n   'DEP_DEL15']\n\n  numerics = list(set(dataframe.columns).difference(categoricals))\n  \n  # Casting the model\n  dataframe = dataframe.select([f.col(feature).cast(\"int\") for feature in numerics] + [f.col(feature).cast(\"string\") for feature in categoricals])\n  \n  # Imputation\n  # Creating seperate lists that include column names to calculate mean and median\n  cols_mean = [\"ALTIMETER_SET\", \"DEW_TEMP\", \"SLP_PRESSURE\", \"TMP_TEMP\"]\n  cols_median = [\"CLOUD_BASE_HEIGHT\", \"CLOUD_COVERAGE\", \"PRECIPITATION\", \"SNOW\", \"VIS_DISTANCE\", \"WND_SPEED\"]\n\n  # replacing null values with mean value\n  for c in cols_mean:\n    c_mean = dataframe.agg({c: 'mean'}).collect()[0][0]\n    dataframe = dataframe.na.fill({c: c_mean})\n\n  # replacing null values with median value\n  for c in cols_median:\n    c_median = dataframe.approxQuantile(c, [0.5],0.1)[0]\n    dataframe = dataframe.na.fill({c: c_median})\n  \n  return dataframe"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["##### Drop Outcome Variables with NULL Values\n\nAs we only have approximately 3700 records with null labels, we decided to drop those records at this stage"],"metadata":{}},{"cell_type":"code","source":["def drop_null_labels(dataframe):\n  \n  \"\"\"\n  As we only have approximately 3700 records with null labels, we decided to drop those records at this stage\n  \n  Parameters:\n  dataframe: dataset for model training and testing\n  \n  Returns:\n  dataframe: dataset for model training and testing\n  \"\"\"\n  \n  dataframe.createOrReplaceTempView('data_model')\n  \n  dataframe = spark.sql(\n  \"\"\"\n  SELECT\n    *\n  FROM\n    data_model\n  WHERE\n    DEP_DEL15 IS NOT NULL\n  \"\"\"\n  )\n  \n  return dataframe"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["##### Split dataset"],"metadata":{}},{"cell_type":"code","source":["def split_dataset(dataframe):\n  \n  \"\"\"\n  Splitting the data into train and test datasets\n   - Year 2015-2018 Train Data\n   - Year 2019 Test Data\n  \n  Parameters:\n  dataframe: dataset for model training and testing\n  \n  Returns:\n  trainRDD: Train Data\n  testRDD: Test Data\n  \"\"\"\n  \n  testRDD = dataframe.filter(\"YEAR = '2019'\")\n  trainRDD = dataframe.filter(\"YEAR != '2019'\")\n  return trainRDD, testRDD"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["##### Gradient Boosted Trees"],"metadata":{}},{"cell_type":"code","source":["def gradient_boosted_trees(trainRDD, testRDD):\n  \n  \"\"\"\n  Training and Testing data on Gradient Boosted Algorithm\n  \n  Parameters:\n  trainRDD: Train Data\n  testRDD: Test Data\n  \n  Returns:\n  prediction\n  \"\"\"  \n  \n  # obtain numeric features \n  numerics_gbt = [feature for (feature, dataType) in trainRDD.dtypes if ((dataType == \"double\") | (dataType == \"int\")) & (feature != \"DEP_DEL15\")]\n\n  # obtain categorical features \n  categoricals_gbt = [feature for (feature, dataType) in trainRDD.dtypes if (dataType == \"string\") & (feature != \"DEP_DEL15\")]\n  \n  # Establish stages for our GBT model\n  inputCol_gbt = [x + \"Index\" for x in categoricals_gbt]\n  indexers_gbt = StringIndexer(inputCols=categoricals_gbt, outputCols=inputCol_gbt, handleInvalid=\"keep\")\n  label_indexers_gbt = StringIndexer(inputCol=\"DEP_DEL15\", outputCol=\"label\")\n\n  featureCols_gbt = inputCol_gbt + numerics_gbt\n\n  # Define vector assemblers\n  vector_gbt = VectorAssembler(inputCols=featureCols_gbt, outputCol=\"features\")\n\n  # Define a GBT model.\n  gbt = GBTClassifier(featuresCol=\"features\",\n                      labelCol=\"label\",\n                      lossType = \"logistic\",\n                      maxIter = 50,\n                      maxDepth = 10,\n                      maxBins = 400)\n\n  # Chain indexer and GBT in a Pipeline\n  stages_gbt = [indexers_gbt, label_indexers_gbt, vector_gbt, gbt]\n  pipeline_gbt = Pipeline(stages=stages_gbt)\n\n  # Train the tuned model and establish our best model\n  cv_gbt_model = pipeline_gbt.fit(trainRDD)\n\n  test_gbt = cv_gbt_model.transform(testRDD)\n\n  evaluator = BinaryClassificationEvaluator()\n\n  tp = test_gbt[(test_gbt.DEP_DEL15 == 1) & (test_gbt.prediction == 1)].count()\n  tn = test_gbt[(test_gbt.DEP_DEL15 == 0) & (test_gbt.prediction == 0)].count()\n  fp = test_gbt[(test_gbt.DEP_DEL15 == 0) & (test_gbt.prediction == 1)].count()\n  fn = test_gbt[(test_gbt.DEP_DEL15 == 1) & (test_gbt.prediction == 0)].count()\n\n  print(\"########### Gradient Boosted Trees ###########\\n\")\n  data = {\"Actual: delay\": [tp, fn], \"Actual: on-time\": [fp, tn]}\n  confusion_matrix = pd.DataFrame.from_dict(data, orient=\"index\", columns=[\"Prediction: delay\", \"Prediction: on-time\"])\n\n  print(\"Test Area Under ROC: \", \"{:.2f}\".format(evaluator.evaluate(test_gbt, {evaluator.metricName: \"areaUnderROC\"})))\n  print(\"Test Area Under Precision-Recall Curve: \", \"{:.2f}\".format(evaluator.evaluate(test_gbt, {evaluator.metricName: \"areaUnderPR\"})))\n\n  print(\"True positive rate: {:.2%}\".format(tp/(tp+fn)))\n  print(\"True negative rate: {:.2%}\".format(tn/(tn+fp)))\n  print(\"False positive rate: {:.2%}\".format(fp/(tn+fp)))\n  print(\"False negative rate: {:.2%}\".format(fn/(tp+fn)))\n  \n  print(\"---------------------------------------\\n\")\n  precision = tp/(tp + fp)\n  print(\"Precision: {:.2%}\".format(precision))\n  recall = tp/(tp + fn)\n  print(\"Recall: {:.2%}\".format(recall))\n\n  f1_score = (2 * precision * recall)/(precision + recall)\n  print(\"F1 Score: {:.2%}\".format(f1_score))\n\n  \n  print(\"########### Confusion Matrix ###########\\n\")\n  print(confusion_matrix)\n  \n  prediction = test_gbt.select(\"prediction\")\n  return prediction"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["## End to End Pipeline"],"metadata":{}},{"cell_type":"code","source":["# End to End Pipeline:\n\ndef end_to_end_pipeline(airlines_path, weather_path, airport_path):\n  \n  \"\"\"\n  Calling the end to end pipeline which transforms data, prints evaluation metrics and returns prediction\n  \n  Parameters:\n  airlines_path: path to airlines dataset\n  weather_path: path to weather dataset\n  airport_path: path to airport dataset\n  \n  Returns: prediction\n  \"\"\"\n  \n  # selected features for weather\n  shorlisted_weather_cols = [\"STATION\", \"DATE\", \"LATITUDE\", 'LONGITUDE', 'NAME', 'REPORT_TYPE', 'CALL_SIGN', 'WND', 'CIG', 'VIS', 'TMP', 'DEW', 'SLP', 'AA1', 'AJ1', 'AT1', 'GA1', 'IA1', 'MA1']\n  \n  # initiate airlines, airport, and weather dataframe\n  airlines_df = spark.read.option(\"header\", \"true\").parquet(airlines_path)\n  airport_df = airport_extract(airport_path)\n  weather_df = weather_extraction(weather_path)\n  \n  # airport transformation\n  airport_df = airport_transform(airport_df)\n  airport_df = airport_selection(airport_df, airlines_df)\n  \n  # airlines transformation \n  airlines_df = airlines_transform(airlines_df)\n  \n  # weather transformation 1\n  weather_df = weather_transformation_reduction(weather_df, shorlisted_weather_cols)\n  \n  # obtain weather coordinates\n  weather_coordinates = weather_coordinates_extract(weather_df)\n  \n  # weather station spatial join\n  weather_df, airport_df = spatial_join(weather_coordinates, airport_df, weather_df)\n  \n  # weather transformation 2\n  weather_df = weather_transformation(weather_df)\n  \n  # weather and airlines transformation 3\n  airlines_df, weather_df = create_timestamp(airlines_df, weather_df)\n  \n  # airport to airline join\n  airlines_df = airport_airline_join(airlines_df, airport_df)\n\n  # weather to airlines left join\n  airlines_df = airlines_weather_leftjoin(airlines_df, weather_df)\n  \n  # Create data model & drop NULL values\n  data_model = create_data_model(airlines_df)\n  data_model = drop_null_labels(data_model)\n  \n  # train, test, validation data split\n  trainRDD, testRDD = split_dataset(data_model)\n  \n  # ML Pipeline\n  trainRDD.cache()\n  testRDD.cache()\n  prediction = gradient_boosted_trees(trainRDD, testRDD)\n  \n  return prediction"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"code","source":["# Calling the end to end pipeline\nairport_path = \"/FileStore/tables/193498910_T_MASTER_CORD.csv\"\nairlines_path = \"dbfs:/mnt/mids-w261/data/datasets_final_project/parquet_airlines_data/*.parquet\"\nweather_path = \"dbfs:/mnt/mids-w261/data/datasets_final_project/weather_data/*.parquet\"\n\nend_to_end_pipeline(airlines_path, weather_path, airport_path)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":39},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":40}],"metadata":{"name":"End-To-End Pipeline","notebookId":887545360132133},"nbformat":4,"nbformat_minor":0}
